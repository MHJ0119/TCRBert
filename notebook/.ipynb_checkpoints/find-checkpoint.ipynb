{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-15 22:00:25 [INFO]: Loaded exp_conf: {'title': 'exp1', 'description': 'Fine-tuning of pre-trained TAPE model in a progressively specialized manner', 'paper': 'exp1', 'model_config': '../config/bert-base/', 'outdir': '../output/exp1', 'train': {'pretrained_model': {'type': 'tape', 'location': '../config/bert-base/'}, 'data_parallel': False, 'backup': 'train.bak.{date}.tar.gz', 'rounds': [{'data': 'dash_vdjdb_mcpas', 'test_size': 0.2, 'batch_size': 128, 'n_epochs': 150, 'n_workers': 12, 'metrics': ['accuracy'], 'optimizer': {'type': 'adam', 'lr': 0.0001}, 'train_bert_encoders': [-10, None], 'early_stopper': {'monitor': 'accuracy', 'patience': 15}, 'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk', 'monitor': 'accuracy', 'save_best_only': True, 'period': 1}, 'result': 'train.{round}.result.json'}, {'data': 'iedb_sars2', 'test_size': 0.2, 'batch_size': 128, 'n_epochs': 100, 'n_workers': 12, 'metrics': ['accuracy'], 'optimizer': {'type': 'adam', 'lr': 0.0001}, 'train_bert_encoders': [-6, None], 'early_stopper': {'monitor': 'accuracy', 'patience': 10}, 'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk', 'monitor': 'accuracy', 'save_best_only': True, 'period': 1}, 'result': 'train.{round}.result.json'}]}, 'eval': {'data_parallel': False, 'batch_size': 128, 'n_workers': 12, 'metrics': ['accuracy', 'f1', 'roc_auc'], 'output_attentions': False, 'tests': [{'data': 'shomuradova', 'result': 'eval.shomuradova.result.json'}, {'data': 'immunecode', 'result': 'eval.immunecode.result.json'}]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'exp1',\n",
       " 'description': 'Fine-tuning of pre-trained TAPE model in a progressively specialized manner',\n",
       " 'paper': 'exp1',\n",
       " 'model_config': '../config/bert-base/',\n",
       " 'outdir': '../output/exp1',\n",
       " 'train': {'pretrained_model': {'type': 'tape',\n",
       "   'location': '../config/bert-base/'},\n",
       "  'data_parallel': False,\n",
       "  'backup': 'train.bak.{date}.tar.gz',\n",
       "  'rounds': [{'data': 'dash_vdjdb_mcpas',\n",
       "    'test_size': 0.2,\n",
       "    'batch_size': 128,\n",
       "    'n_epochs': 150,\n",
       "    'n_workers': 12,\n",
       "    'metrics': ['accuracy'],\n",
       "    'optimizer': {'type': 'adam', 'lr': 0.0001},\n",
       "    'train_bert_encoders': [-10, None],\n",
       "    'early_stopper': {'monitor': 'accuracy', 'patience': 15},\n",
       "    'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk',\n",
       "     'monitor': 'accuracy',\n",
       "     'save_best_only': True,\n",
       "     'period': 1},\n",
       "    'result': 'train.{round}.result.json'},\n",
       "   {'data': 'iedb_sars2',\n",
       "    'test_size': 0.2,\n",
       "    'batch_size': 128,\n",
       "    'n_epochs': 100,\n",
       "    'n_workers': 12,\n",
       "    'metrics': ['accuracy'],\n",
       "    'optimizer': {'type': 'adam', 'lr': 0.0001},\n",
       "    'train_bert_encoders': [-6, None],\n",
       "    'early_stopper': {'monitor': 'accuracy', 'patience': 10},\n",
       "    'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk',\n",
       "     'monitor': 'accuracy',\n",
       "     'save_best_only': True,\n",
       "     'period': 1},\n",
       "    'result': 'train.{round}.result.json'}]},\n",
       " 'eval': {'data_parallel': False,\n",
       "  'batch_size': 128,\n",
       "  'n_workers': 12,\n",
       "  'metrics': ['accuracy', 'f1', 'roc_auc'],\n",
       "  'output_attentions': False,\n",
       "  'tests': [{'data': 'shomuradova', 'result': 'eval.shomuradova.result.json'},\n",
       "   {'data': 'immunecode', 'result': 'eval.immunecode.result.json'}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import logging.config\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from enum import auto\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.display import display\n",
    "\n",
    "rootdir = '/home/hym/trunk/TCRBert'\n",
    "workdir = '%s/notebook' % rootdir\n",
    "datadir = '%s/data' % rootdir\n",
    "srcdir = '%s/tcrbert' % rootdir\n",
    "outdir = '%s/output' % rootdir\n",
    "\n",
    "os.chdir(workdir)\n",
    "\n",
    "sys.path.append(rootdir)\n",
    "sys.path.append(srcdir)\n",
    "\n",
    "from tcrbert.exp import Experiment\n",
    "from tcrbert.predlistener import PredResultRecoder\n",
    "\n",
    "\n",
    "# Display\n",
    "pd.set_option('display.max.rows', 2000)\n",
    "pd.set_option('display.max.columns', 2000)\n",
    "\n",
    "# Logger\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.config.fileConfig('../config/logging.conf')\n",
    "logger = logging.getLogger('tcrbert')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Target experiment\n",
    "exp_key = 'exp1'\n",
    "experiment = Experiment.from_key(exp_key)\n",
    "\n",
    "exp_conf = experiment.exp_conf\n",
    "\n",
    "display(exp_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find target aa position in the epitope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     24
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-15 22:03:14 [INFO]: >>>Begin train 0\n",
      "2021-11-15 22:03:14 [INFO]: ======================\n",
      "2021-11-15 22:03:14 [INFO]: Begin train at 2021-11-15 22:03:14.458654\n",
      "2021-11-15 22:03:14 [INFO]: Loading the TAPE pretrained model from ../config/bert-base/\n",
      "2021-11-15 22:03:18 [INFO]: Start 2 train rounds of exp1 at 2021-11-15 22:03:14.458654\n",
      "2021-11-15 22:03:18 [INFO]: train_conf: {'pretrained_model': {'type': 'tape', 'location': '../config/bert-base/'}, 'data_parallel': False, 'backup': 'train.bak.{date}.tar.gz', 'rounds': [{'data': 'dash_vdjdb_mcpas', 'test_size': 0.2, 'batch_size': 128, 'n_epochs': 150, 'n_workers': 12, 'metrics': ['accuracy'], 'optimizer': {'type': 'adam', 'lr': 0.0001}, 'train_bert_encoders': [-10, None], 'early_stopper': {'monitor': 'accuracy', 'patience': 15}, 'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk', 'monitor': 'accuracy', 'save_best_only': True, 'period': 1}, 'result': 'train.{round}.result.json'}, {'data': 'iedb_sars2', 'test_size': 0.2, 'batch_size': 128, 'n_epochs': 100, 'n_workers': 12, 'metrics': ['accuracy'], 'optimizer': {'type': 'adam', 'lr': 0.0001}, 'train_bert_encoders': [-6, None], 'early_stopper': {'monitor': 'accuracy', 'patience': 10}, 'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk', 'monitor': 'accuracy', 'save_best_only': True, 'period': 1}, 'result': 'train.{round}.result.json'}]}\n",
      "2021-11-15 22:03:18 [INFO]: Start 0 train round using data: dash_vdjdb_mcpas, round_conf: {'data': 'dash_vdjdb_mcpas', 'test_size': 0.2, 'batch_size': 128, 'n_epochs': 150, 'n_workers': 12, 'metrics': ['accuracy'], 'optimizer': {'type': 'adam', 'lr': 0.0001}, 'train_bert_encoders': [-10, None], 'early_stopper': {'monitor': 'accuracy', 'patience': 15}, 'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk', 'monitor': 'accuracy', 'save_best_only': True, 'period': 1}, 'result': 'train.{round}.result.json'}\n",
      "2021-11-15 22:03:19 [INFO]: The bert encoders to be trained: [-10, None]\n",
      "2021-11-15 22:03:19 [INFO]: ======================\n",
      "2021-11-15 22:03:19 [INFO]: Begin training...\n",
      "2021-11-15 22:03:19 [INFO]: use_cuda, device: True, cuda:0\n",
      "2021-11-15 22:03:19 [INFO]: train.n_data: 20110, test.n_data: 5028\n",
      "2021-11-15 22:03:19 [INFO]: optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "2021-11-15 22:03:19 [INFO]: evaluator: <tcrbert.model.BertTCREpitopeModel.PredictionEvaluator object at 0x7f8295048b50>\n",
      "2021-11-15 22:03:19 [INFO]: n_epochs: 150\n",
      "2021-11-15 22:03:19 [INFO]: train.batch_size: 128\n",
      "2021-11-15 22:03:19 [INFO]: test.batch_size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training in epoch 0/150: 100%|██████████| 158/158 [03:18<00:00,  1.26s/batch]\n",
      "Validating in epoch 0/150: 100%|██████████| 40/40 [00:37<00:00,  1.08batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-15 22:07:15 [INFO]: [EvalScoreRecoder]: In epoch 0/150, loss train score: 0.6940721258332457, val score: 0.6900263592600823\n",
      "2021-11-15 22:07:15 [INFO]: [EvalScoreRecoder]: In epoch 0/150, accuracy train score: 0.5162607368896927, val score: 0.5508680555555555\n",
      "2021-11-15 22:07:15 [INFO]: [EarlyStopper]: In epoch 0/150, accuracy score: 0.5508680555555555, best accuracy score: -inf;update best score to 0.5508680555555555\n",
      "2021-11-15 22:07:15 [INFO]: [ModelCheckpoint]: Checkpoint at epoch 0: accuracy improved from -inf to 0.5508680555555555, saving model to ../output/exp1/train.0.model_0.chk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training in epoch 1/150: 100%|██████████| 158/158 [03:21<00:00,  1.27s/batch]\n",
      "Validating in epoch 1/150: 100%|██████████| 40/40 [00:36<00:00,  1.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-15 22:11:14 [INFO]: [EvalScoreRecoder]: In epoch 1/150, loss train score: 0.6928321036356914, val score: 0.6936042934656144\n",
      "2021-11-15 22:11:14 [INFO]: [EvalScoreRecoder]: In epoch 1/150, accuracy train score: 0.5163808205244123, val score: 0.5004991319444445\n",
      "2021-11-15 22:11:14 [INFO]: [EarlyStopper]: In epoch 1/150, accuracy score: 0.5004991319444445, best accuracy score: 0.5508680555555555;accuracy score was not improved\n",
      "2021-11-15 22:11:14 [INFO]: [EarlyStopper]: Current wait count: 1, patience: 15\n",
      "2021-11-15 22:11:14 [INFO]: [ModelCheckpoint]: Checkpoint at epoch 1: accuracy did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training in epoch 2/150:  85%|████████▍ | 134/158 [02:49<00:30,  1.25s/batch]"
     ]
    }
   ],
   "source": [
    "from tcrbert.dataset import TCREpitopeSentenceDataset, CN\n",
    "from collections import OrderedDict, Counter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "epitope = 'YLQPRTFLL'\n",
    "epitope_len = len(epitope)\n",
    "\n",
    "sh_ds = TCREpitopeSentenceDataset.from_key('shomuradova')\n",
    "sh_df = sh_ds.df_enc\n",
    "im_ds = TCREpitopeSentenceDataset.from_key('immunecode')\n",
    "\n",
    "# Remove duplicated CDR3beta seqs with Shomuradova\n",
    "im_ds.df_enc = im_ds.df_enc[\n",
    "        im_ds.df_enc[CN.cdr3b].map(lambda seq: seq not in sh_df[CN.cdr3b].values)\n",
    "]\n",
    "im_df = im_ds.df_enc\n",
    "\n",
    "n_found = 0\n",
    "n_train = 0\n",
    "metrics = ['accuracy', 'f1', 'roc_auc']\n",
    "\n",
    "target_attn_pos = 3\n",
    "\n",
    "while(n_found < 5):\n",
    "    found = True\n",
    "\n",
    "    logger.info('>>>Begin train %s' % n_train)\n",
    "    \n",
    "    experiment.train()\n",
    "    \n",
    "    logger.info('>>>Done to train %s' % n_train)\n",
    "    n_train = n_train + 1\n",
    "    \n",
    "    for i in range(experiment.n_train_rounds):\n",
    "        train_result = experiment.get_train_result(i)\n",
    "        logger.info('Round %s train results======================' % i)\n",
    "        logger.info('n_epochs: %s' % train_result['n_epochs'])\n",
    "        logger.info('stopped_epoch: %s' % train_result['stopped_epoch'])\n",
    "        logger.info('best_epoch: %s' % train_result['best_epoch'])\n",
    "        logger.info('best_score: %s' % train_result['best_score'])\n",
    "        logger.info('best_chk: %s' % train_result['best_chk'])\n",
    "    \n",
    "    \n",
    "    model = experiment.load_eval_model()\n",
    "    eval_recoder = PredResultRecoder(output_attentions=True, output_hidden_states=True)\n",
    "    model.add_pred_listener(eval_recoder)    \n",
    "    \n",
    "    for ds, max_cum_ratio in zip([sh_ds, im_ds], [0.9, 0.85]):\n",
    "        df = ds.df_enc\n",
    "        data_loader = DataLoader(ds, batch_size=len(ds), shuffle=False, num_workers=2)\n",
    "        logger.info('Predicting for %s' % ds.name)\n",
    "        model.predict(data_loader=data_loader, metrics=metrics)\n",
    "        logger.info('Performace score_map for %s: %s' % (ds.name, eval_recoder.result_map['score_map']))\n",
    "        \n",
    "        output_labels = np.array(eval_recoder.result_map['output_labels'])\n",
    "        \n",
    "        # Select target CDR3b sequences with most common lengths\n",
    "        pos_indices = np.where(output_labels == 1)[0]\n",
    "        # print('pos_indices: %s(%s)' % (pos_indices, str(pos_indices.shape)))\n",
    "        pos_cdr3b = df[CN.cdr3b].values[pos_indices]\n",
    "\n",
    "        lens, cnts = zip(*sorted(Counter(map(lambda x: len(x), pos_cdr3b)).items()))\n",
    "        lens = np.array(lens)\n",
    "        cnts = np.array(cnts)\n",
    "\n",
    "        # Select target indices by cdr3b sequence lenghts\n",
    "        target_index_map = OrderedDict()\n",
    "        order = np.argsort(cnts)[::-1]\n",
    "        cum_cnt = 0\n",
    "        for cur_len, cur_cnt in zip(lens[order], cnts[order]):\n",
    "            cum_cnt += cur_cnt\n",
    "            cum_ratio = cum_cnt/pos_indices.shape[0]\n",
    "            if cum_ratio < max_cum_ratio:\n",
    "                target_indices = np.where((output_labels == 1) & (df[CN.cdr3b].map(lambda x: len(x) == cur_len)))[0]\n",
    "                logger.debug('target_indices for %s: %s(%s)' % (cur_len, target_indices, target_indices.shape[0]))\n",
    "                target_index_map[cur_len] = target_indices\n",
    "        \n",
    "        # Investigate attention weights\n",
    "        attentions = eval_recoder.result_map['attentions']\n",
    "        # attentions.shape: (n_layers, n_data, n_heads, max_len, max_len)\n",
    "        logger.info('attentions.shape: %s' % str(attentions.shape))\n",
    "\n",
    "        for i, (cur_len, cur_indices) in enumerate(target_index_map.items()):\n",
    "            attns = attentions[:, cur_indices]\n",
    "            sent_len = epitope_len + cur_len\n",
    "\n",
    "            # Marginalized position-wise attentions by mean\n",
    "            attns = np.mean(attns, axis=(0, 1, 2, 3))[1:sent_len+1]\n",
    "            logger.info('Marginalized attns for cdr3b %s: %s (%s)' % (cur_len, attns, str(attns.shape)))\n",
    "            \n",
    "            epi_attns = attns[:epitope_len]\n",
    "            cur_max_attn_pos = np.argmax(epi_attns)\n",
    "            logger.info('Current max epitope attention weight: %s at %s' % (epi_attns[cur_max_attn_pos], \n",
    "                                                                            cur_max_attn_pos))\n",
    "            if target_attn_pos != cur_max_attn_pos:\n",
    "                found = False\n",
    " \n",
    "    if found:\n",
    "        logger.info('>>>>>Found it!, backup train results, n_found: %s' % n_found)\n",
    "        experiment.backup_train_results()\n",
    "        n_found = n_found + 1\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "639px",
    "left": "1740px",
    "right": "20px",
    "top": "120px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
